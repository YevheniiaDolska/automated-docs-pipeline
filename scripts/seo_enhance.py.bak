#!/usr/bin/env python3
"""
Enhanced SEO optimization for documentation.
Adds structured data, canonical URLs, and sitemap generation.
"""

import json
import yaml
import re
from pathlib import Path
from datetime import datetime
from urllib.parse import quote

class SEOEnhancer:
    def __init__(self, base_url="https://docs.example.com"):
        self.base_url = base_url.rstrip('/')

    def generate_structured_data(self, filepath, frontmatter, content):
        """Generate JSON-LD structured data for better search engine understanding."""

        # Extract main heading
        heading_match = re.search(r'^#\s+(.+)$', content, re.MULTILINE)
        main_heading = heading_match.group(1) if heading_match else frontmatter.get('title', '')

        # Determine article type
        content_type = frontmatter.get('content_type', 'article')
        article_type = {
            'how-to': 'HowTo',
            'tutorial': 'TechArticle',
            'troubleshooting': 'FAQPage',
            'reference': 'TechArticle',
            'concept': 'Article'
        }.get(content_type, 'Article')

        # Base structured data
        structured_data = {
            "@context": "https://schema.org",
            "@type": article_type,
            "headline": main_heading,
            "description": frontmatter.get('description', ''),
            "url": f"{self.base_url}/{str(filepath).replace('docs/', '').replace('.md', '')}",
            "dateModified": frontmatter.get('last_reviewed', datetime.now().isoformat()),
            "author": {
                "@type": "Organization",
                "name": "n8n Documentation Team"
            }
        }

        # Add how-to specific fields
        if article_type == 'HowTo':
            steps = self._extract_steps(content)
            if steps:
                structured_data['step'] = steps

            # Extract time estimate
            time_match = re.search(r'(\d+)\s*(?:min|minute)', content, re.IGNORECASE)
            if time_match:
                structured_data['totalTime'] = f"PT{time_match.group(1)}M"

        # Add FAQ specific fields
        elif article_type == 'FAQPage':
            qa_pairs = self._extract_qa_pairs(content)
            if qa_pairs:
                structured_data['mainEntity'] = qa_pairs

        # Add breadcrumb
        breadcrumb = self._generate_breadcrumb(filepath)
        if breadcrumb:
            structured_data['breadcrumb'] = breadcrumb

        return structured_data

    def _extract_steps(self, content):
        """Extract numbered steps for HowTo schema."""
        steps = []
        step_pattern = r'^\d+\.\s+(.+?)(?=^\d+\.|^#{1,6}\s|$)'
        matches = re.findall(step_pattern, content, re.MULTILINE | re.DOTALL)

        for i, step_text in enumerate(matches, 1):
            # Clean up step text
            step_text = re.sub(r'\n+', ' ', step_text).strip()
            steps.append({
                "@type": "HowToStep",
                "position": i,
                "name": f"Step {i}",
                "text": step_text[:500]  # Limit length
            })

        return steps

    def _extract_qa_pairs(self, content):
        """Extract Q&A pairs for FAQ schema."""
        qa_pairs = []

        # Look for Problem/Solution patterns
        problem_pattern = r'(?:Problem|Issue|Error):\s*(.+?)(?:Solution|Fix|Resolution):\s*(.+?)(?=(?:Problem|Issue|Error):|$)'
        matches = re.findall(problem_pattern, content, re.IGNORECASE | re.DOTALL)

        for problem, solution in matches:
            qa_pairs.append({
                "@type": "Question",
                "name": problem.strip()[:200],
                "acceptedAnswer": {
                    "@type": "Answer",
                    "text": solution.strip()[:500]
                }
            })

        return qa_pairs

    def _generate_breadcrumb(self, filepath):
        """Generate breadcrumb structured data."""
        parts = Path(filepath).parts[1:-1]  # Skip 'docs' and filename
        if not parts:
            return None

        items = []
        current_path = ""

        for i, part in enumerate(parts, 1):
            current_path += f"/{part}"
            items.append({
                "@type": "ListItem",
                "position": i,
                "name": part.replace('-', ' ').title(),
                "item": f"{self.base_url}{current_path}"
            })

        return {
            "@type": "BreadcrumbList",
            "itemListElement": items
        }

    def generate_meta_tags(self, frontmatter, filepath):
        """Generate comprehensive meta tags for SEO."""

        title = frontmatter.get('title', '')
        description = frontmatter.get('description', '')

        # Generate canonical URL
        canonical = f"{self.base_url}/{str(filepath).replace('docs/', '').replace('.md', '')}"

        meta_tags = {
            # Basic meta tags
            'title': title,
            'description': description,
            'canonical': canonical,

            # Open Graph tags (for social sharing)
            'og:title': title,
            'og:description': description,
            'og:url': canonical,
            'og:type': 'article',
            'og:site_name': 'n8n Documentation',

            # Twitter Card tags
            'twitter:card': 'summary',
            'twitter:title': title,
            'twitter:description': description,

            # Additional SEO tags
            'robots': 'index, follow',
            'author': 'n8n Documentation Team'
        }

        # Add product-specific tags if applicable
        if 'product' in frontmatter:
            meta_tags['article:section'] = frontmatter['product']

        # Add tags/keywords
        if 'tags' in frontmatter:
            meta_tags['keywords'] = ', '.join(frontmatter['tags'])
            meta_tags['article:tag'] = frontmatter['tags']

        return meta_tags

    def generate_sitemap_entry(self, filepath, frontmatter):
        """Generate sitemap entry for the file."""

        # Determine priority based on content type and path
        priority = 0.5  # default

        if 'index' in filepath.name:
            priority = 0.9
        elif frontmatter.get('content_type') == 'reference':
            priority = 0.8
        elif 'getting-started' in str(filepath):
            priority = 0.7
        elif frontmatter.get('content_type') == 'troubleshooting':
            priority = 0.4

        # Determine change frequency
        last_reviewed = frontmatter.get('last_reviewed', '')
        if last_reviewed:
            days_old = (datetime.now() - datetime.fromisoformat(last_reviewed)).days
            if days_old < 30:
                changefreq = 'weekly'
            elif days_old < 90:
                changefreq = 'monthly'
            else:
                changefreq = 'yearly'
        else:
            changefreq = 'monthly'

        return {
            'loc': f"{self.base_url}/{str(filepath).replace('docs/', '').replace('.md', '')}",
            'lastmod': last_reviewed or datetime.now().isoformat()[:10],
            'changefreq': changefreq,
            'priority': priority
        }

def extract_frontmatter(text):
    """Extract frontmatter from markdown."""
    if not text.startswith('---'):
        return {}, text
    parts = text.split('---', 2)
    if len(parts) < 3:
        return {}, text
    try:
        fm = yaml.safe_load(parts[1]) or {}
        return fm, parts[2]
    except yaml.YAMLError:
        return {}, text

def inject_seo_enhancements(filepath, dry_run=False):
    """Add SEO enhancements to a markdown file."""

    enhancer = SEOEnhancer()
    content = filepath.read_text(encoding='utf-8')
    frontmatter, body = extract_frontmatter(content)

    # Generate SEO data
    structured_data = enhancer.generate_structured_data(filepath, frontmatter, body)
    meta_tags = enhancer.generate_meta_tags(frontmatter, filepath)
    sitemap_entry = enhancer.generate_sitemap_entry(filepath, frontmatter)

    # Add to frontmatter
    frontmatter['seo'] = {
        'meta_tags': meta_tags,
        'structured_data': structured_data,
        'sitemap': sitemap_entry
    }

    if dry_run:
        print(f"Would enhance {filepath}:")
        print(f"  + Structured data: {structured_data.get('@type')}")
        print(f"  + Meta tags: {len(meta_tags)} tags")
        print(f"  + Sitemap priority: {sitemap_entry['priority']}")
        return

    # Write back
    yaml_str = yaml.dump(frontmatter, default_flow_style=False, sort_keys=False)
    new_content = f"---\n{yaml_str}---\n{body}"
    filepath.write_text(new_content, encoding='utf-8')

    print(f"✅ {filepath}: SEO enhanced")

def generate_sitemap(docs_dir, output_file='sitemap.xml'):
    """Generate a complete sitemap.xml file."""

    enhancer = SEOEnhancer()
    entries = []

    for md_file in Path(docs_dir).rglob('*.md'):
        if md_file.name.startswith('_'):
            continue

        content = md_file.read_text(encoding='utf-8')
        frontmatter, _ = extract_frontmatter(content)
        entry = enhancer.generate_sitemap_entry(md_file, frontmatter)
        entries.append(entry)

    # Generate XML
    xml_content = '<?xml version="1.0" encoding="UTF-8"?>\n'
    xml_content += '<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">\n'

    for entry in sorted(entries, key=lambda x: x['priority'], reverse=True):
        xml_content += '  <url>\n'
        xml_content += f"    <loc>{entry['loc']}</loc>\n"
        xml_content += f"    <lastmod>{entry['lastmod']}</lastmod>\n"
        xml_content += f"    <changefreq>{entry['changefreq']}</changefreq>\n"
        xml_content += f"    <priority>{entry['priority']}</priority>\n"
        xml_content += '  </url>\n'

    xml_content += '</urlset>'

    Path(output_file).write_text(xml_content, encoding='utf-8')
    print(f"✅ Generated sitemap with {len(entries)} URLs: {output_file}")

def main():
    import argparse
    parser = argparse.ArgumentParser(description='Enhance documentation SEO')
    parser.add_argument('--enhance', metavar='PATH', help='Enhance SEO for file/directory')
    parser.add_argument('--sitemap', action='store_true', help='Generate sitemap.xml')
    parser.add_argument('--dry-run', action='store_true', help='Show changes without applying')
    args = parser.parse_args()

    if args.enhance:
        path = Path(args.enhance)
        if path.is_file():
            inject_seo_enhancements(path, args.dry_run)
        else:
            for md_file in path.rglob('*.md'):
                if not md_file.name.startswith('_'):
                    inject_seo_enhancements(md_file, args.dry_run)

    if args.sitemap:
        generate_sitemap('docs')

if __name__ == '__main__':
    main()